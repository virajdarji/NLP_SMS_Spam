{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce1125a",
   "metadata": {},
   "source": [
    "# Viraj Nishesh Darji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e7b71c",
   "metadata": {},
   "source": [
    "## Reading file and doing basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57d3aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a81462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms = pd.read_table('SMSSpamCollection', header=None)\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2423d2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0                       1\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2933a0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sms[0]\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111e531",
   "metadata": {},
   "source": [
    "Using LabelEncoder we can setting spam and ham (no spam) to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2b46db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c5c576a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "sms.columns=['label', 'msg']\n",
    "sms[\"length\"] = sms[\"msg\"].apply(len)\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82304b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIRAJ\\Anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='length'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEGCAYAAACJnEVTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAATmElEQVR4nO3dfYxc133e8e9TMqIlK4IkaCUwJFXSBWGHElq/sJQcA4ZRJRFhG6aAQgitOKYTFUQMNXGSpraYAFFbgKjaBGliIBLKyC8yopcQigsRLpxYYGwYRRXKqxdHomhGjNmQa9Hium4cOQXoiPn1jzmqJqvhy84sl9Ke7wcYzL2/e+69Zw65z1zcuXMnVYUkqQ//6Hx3QJK0eAx9SeqIoS9JHTH0Jakjhr4kdWT5+e7AmVxxxRW1du3a890NSXpdefzxx79TVVNz66/50F+7di3T09PnuxuS9LqS5K9G1T29I0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHXnNfyP3XLh/35GR9Vuuu3qReyJJi8sjfUnqiKEvSR0x9CWpI4a+JHXkjKGf5NNJjid5ZsSyX01SSa4Yqu1IcijJwSQ3DtXfkeTptuyTSbJwL0OSdDbO5kj/s8DmucUka4CfAI4M1TYAW4Fr2jp3JVnWFt8NbAfWt8ertilJOrfOGPpV9VXguyMW/Rfg40AN1bYAD1bViao6DBwCNiVZCVxSVY9WVQGfA26atPOSpPkZ65x+kg8A36qqr89ZtAo4OjQ/02qr2vTc+qm2vz3JdJLp2dnZcbooSRph3qGf5CLg14HfGLV4RK1OUx+pqnZV1caq2jg19aqfeJQkjWmcb+T+E2Ad8PX2Wexq4Ikkmxgcwa8ZarsaeL7VV4+oS5IW0byP9Kvq6aq6sqrWVtVaBoH+9qr6NrAH2JpkRZJ1DD6wfayqjgEvJrm+XbXzYeDhhXsZkqSzcTaXbD4APAq8OclMkltP1baq9gO7gWeBPwZuq6qTbfFHgXsYfLj7l8AXJ+y7JGmeznh6p6o+eIbla+fM7wR2jmg3DVw7z/5JkhaQ38iVpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOnI2P4z+6STHkzwzVPvNJN9I8udJ/luSS4eW7UhyKMnBJDcO1d+R5Om27JNJsuCvRpJ0WmdzpP9ZYPOc2iPAtVX1T4G/AHYAJNkAbAWuaevclWRZW+duYDuwvj3mblOSdI6dMfSr6qvAd+fUvlRVL7XZPwNWt+ktwINVdaKqDgOHgE1JVgKXVNWjVVXA54CbFug1SJLO0kKc0/854IttehVwdGjZTKutatNz6yMl2Z5kOsn07OzsAnRRkgQThn6SXwdeAu57uTSiWZ2mPlJV7aqqjVW1cWpqapIuSpKGLB93xSTbgPcDN7RTNjA4gl8z1Gw18Hyrrx5RlyQtorGO9JNsBj4BfKCq/u/Qoj3A1iQrkqxj8IHtY1V1DHgxyfXtqp0PAw9P2HdJ0jyd8Ug/yQPAe4ArkswAdzC4WmcF8Ei78vLPqurnq2p/kt3AswxO+9xWVSfbpj7K4EqgCxl8BvBFJEmL6oyhX1UfHFH+1Gna7wR2jqhPA9fOq3eSpAXlN3IlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjpwx9JN8OsnxJM8M1S5P8kiS59rzZUPLdiQ5lORgkhuH6u9I8nRb9sm0X1SXJC2esznS/yyweU7tdmBvVa0H9rZ5kmwAtgLXtHXuSrKsrXM3sB1Y3x5ztylJOsfOGPpV9VXgu3PKW4B72/S9wE1D9Qer6kRVHQYOAZuSrAQuqapHq6qAzw2tI0laJOOe07+qqo4BtOcrW30VcHSo3UyrrWrTc+uSpEW0fIG3N+o8fZ2mPnojyXYGp4K4+uqrF6ZnZ+H+fUdG1m+5bvH6IEnn0rhH+i+0Uza05+OtPgOsGWq3Gni+1VePqI9UVbuqamNVbZyamhqzi5KkucYN/T3Atja9DXh4qL41yYok6xh8YPtYOwX0YpLr21U7Hx5aR5K0SM54eifJA8B7gCuSzAB3AHcCu5PcChwBbgaoqv1JdgPPAi8Bt1XVybapjzK4EuhC4IvtIUlaRGcM/ar64CkW3XCK9juBnSPq08C18+qdJGlB+Y1cSeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZKLQT/LLSfYneSbJA0nekOTyJI8kea49XzbUfkeSQ0kOJrlx8u5LkuZj7NBPsgr4RWBjVV0LLAO2ArcDe6tqPbC3zZNkQ1t+DbAZuCvJssm6L0maj0lP7ywHLkyyHLgIeB7YAtzblt8L3NSmtwAPVtWJqjoMHAI2Tbh/SdI8jB36VfUt4LeAI8Ax4HtV9SXgqqo61tocA65sq6wCjg5tYqbVXiXJ9iTTSaZnZ2fH7aIkaY5JTu9cxuDofR3wI8Abk3zodKuMqNWohlW1q6o2VtXGqampcbsoSZpjktM7Pw4crqrZqvo74PPAjwEvJFkJ0J6Pt/YzwJqh9VczOB0kSVokk4T+EeD6JBclCXADcADYA2xrbbYBD7fpPcDWJCuSrAPWA49NsH9J0jwtH3fFqtqX5CHgCeAl4ElgF3AxsDvJrQzeGG5u7fcn2Q0829rfVlUnJ+y/JGkexg59gKq6A7hjTvkEg6P+Ue13Ajsn2ackaXx+I1eSOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdWSiWyvr3Lt/35GR9Vuuu3qReyJpKfBIX5I6YuhLUkcMfUnqiKEvSR2ZKPSTXJrkoSTfSHIgyTuTXJ7kkSTPtefLhtrvSHIoycEkN07efUnSfEx6pP+7wB9X1VuAfwYcAG4H9lbVemBvmyfJBmArcA2wGbgrybIJ9y9JmoexL9lMcgnwbuAjAFX1A+AHSbYA72nN7gW+AnwC2AI8WFUngMNJDgGbgEfH7cNi8bJJSUvFJEf6bwJmgc8keTLJPUneCFxVVccA2vOVrf0q4OjQ+jOt9ipJtieZTjI9Ozs7QRclScMmCf3lwNuBu6vqbcDf0k7lnEJG1GpUw6raVVUbq2rj1NTUBF2UJA2bJPRngJmq2tfmH2LwJvBCkpUA7fn4UPs1Q+uvBp6fYP+SpHkaO/Sr6tvA0SRvbqUbgGeBPcC2VtsGPNym9wBbk6xIsg5YDzw27v4lSfM36b13fgG4L8kFwDeBn2XwRrI7ya3AEeBmgKran2Q3gzeGl4DbqurkhPuXJM3DRKFfVU8BG0csuuEU7XcCOyfZpyRpfH4jV5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkUl/Oatr9+87MrJ+y3VXL3JPJOnseKQvSR0x9CWpI4a+JHVk4tBPsizJk0m+0OYvT/JIkufa82VDbXckOZTkYJIbJ923JGl+FuJI/2PAgaH524G9VbUe2NvmSbIB2ApcA2wG7kqybAH2L0k6SxNdvZNkNfA+YCfwK628BXhPm74X+ArwiVZ/sKpOAIeTHAI2AY9O0oel4lRXAknSQpr0SP93gI8Dfz9Uu6qqjgG05ytbfRVwdKjdTKu9SpLtSaaTTM/Ozk7YRUnSy8YO/STvB45X1eNnu8qIWo1qWFW7qmpjVW2cmpoat4uSpDkmOb3zLuADSd4LvAG4JMkfAC8kWVlVx5KsBI639jPAmqH1VwPPT7B/SdI8jX2kX1U7qmp1Va1l8AHtn1bVh4A9wLbWbBvwcJveA2xNsiLJOmA98NjYPZckzdu5uA3DncDuJLcCR4CbAapqf5LdwLPAS8BtVXXyHOxfknQKCxL6VfUVBlfpUFX/G7jhFO12MrjSR5J0HviNXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRfy5xEXlTNUnnm0f6ktQRj/TPAY/oJb1WeaQvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSNjh36SNUm+nORAkv1JPtbqlyd5JMlz7fmyoXV2JDmU5GCSGxfiBUiSzt4kR/ovAf+mqn4UuB64LckG4HZgb1WtB/a2edqyrcA1wGbgriTLJum8JGl+xg79qjpWVU+06ReBA8AqYAtwb2t2L3BTm94CPFhVJ6rqMHAI2DTu/iVJ87cg5/STrAXeBuwDrqqqYzB4YwCubM1WAUeHVptptVHb255kOsn07OzsQnRRksQChH6Si4E/An6pqv7mdE1H1GpUw6raVVUbq2rj1NTUpF2UJDUThX6SH2IQ+PdV1edb+YUkK9vylcDxVp8B1gytvhp4fpL9S5LmZ5KrdwJ8CjhQVb89tGgPsK1NbwMeHqpvTbIiyTpgPfDYuPuXJM3fJL+c9S7gZ4CnkzzVar8G3AnsTnIrcAS4GaCq9ifZDTzL4Mqf26rq5AT7lyTN09ihX1X/g9Hn6QFuOMU6O4Gd4+5zvvzZQkn6h/xGriR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6MsldNnUenepmcrdcd/Ui90TS64lH+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0JakjXrK5xHgpp6TTWfTQT7IZ+F1gGXBPVd252H3QK3yTkPqyqKGfZBnwe8BPADPA15LsqapnF7MfPTpVuM+3/aneDHzzkF4fFvtIfxNwqKq+CZDkQWALYOi/TizUm8c4ztcbjm9oWkoWO/RXAUeH5meA6+Y2SrId2N5mv5/k4Bj7ugL4zhjrLUVLYix+emHaL9hYzLc/r0FL4v/FAlmKY/GPRxUXO/QzolavKlTtAnZNtKNkuqo2TrKNpcKxeIVj8QrH4hU9jcViX7I5A6wZml8NPL/IfZCkbi126H8NWJ9kXZILgK3AnkXugyR1a1FP71TVS0n+NfAnDC7Z/HRV7T9Hu5vo9NAS41i8wrF4hWPxim7GIlWvOqUuSVqivA2DJHXE0JekjizJ0E+yOcnBJIeS3H6++3MuJVmT5MtJDiTZn+RjrX55kkeSPNeeLxtaZ0cbm4NJbjx/vT83kixL8mSSL7T5nsfi0iQPJflG+z/yzh7HI8kvt7+PZ5I8kOQNPY4DAFW1pB4MPiD+S+BNwAXA14EN57tf5/D1rgTe3qZ/GPgLYAPwn4HbW/124D+16Q1tTFYA69pYLTvfr2OBx+RXgPuBL7T5nsfiXuBftekLgEt7Gw8GXwo9DFzY5ncDH+ltHF5+LMUj/f9/q4eq+gHw8q0elqSqOlZVT7TpF4EDDP6Tb2HwB097vqlNbwEerKoTVXUYOMRgzJaEJKuB9wH3DJV7HYtLgHcDnwKoqh9U1V/T53gsBy5Mshy4iMH3g3ochyUZ+qNu9bDqPPVlUSVZC7wN2AdcVVXHYPDGAFzZmi318fkd4OPA3w/Veh2LNwGzwGfa6a57kryRzsajqr4F/BZwBDgGfK+qvkRn4/CypRj6Z3Wrh6UmycXAHwG/VFV/c7qmI2pLYnySvB84XlWPn+0qI2pLYiya5cDbgbur6m3A3zI4jXEqS3I82rn6LQxO1fwI8MYkHzrdKiNqr/txeNlSDP3ubvWQ5IcYBP59VfX5Vn4hycq2fCVwvNWX8vi8C/hAkv/F4LTev0jyB/Q5FjB4fTNVta/NP8TgTaC38fhx4HBVzVbV3wGfB36M/sYBWJqh39WtHpKEwTnbA1X120OL9gDb2vQ24OGh+tYkK5KsA9YDjy1Wf8+lqtpRVaurai2Df/c/raoP0eFYAFTVt4GjSd7cSjcwuI15b+NxBLg+yUXt7+UGBp999TYOwBL8ucRa3Fs9vBa8C/gZ4OkkT7XarwF3AruT3MrgP/3NAFW1P8luBn/8LwG3VdXJRe/14up5LH4BuK8dAH0T+FkGB3vdjEdV7UvyEPAEg9f1JIPbLlxMR+PwMm/DIEkdWYqndyRJp2DoS1JHDH1J6oihL0kdMfQlqSOGvrqW5PvnYJtvTfLeofl/l+RXF3o/0jgMfWnhvRV475kaSeeDoS81Sf5tkq8l+fMk/77V1rb70P9+ux/7l5Jc2Jb989b20SS/2e7VfgHwH4CfSvJUkp9qm9+Q5CtJvpnkF8/TS5QMfQkgyU8y+Lr9JgZH6u9I8u62eD3we1V1DfDXwL9s9c8AP19V7wROwuD2xcBvAH9YVW+tqj9sbd8C3Ni2f0e7X5K06Ax9aeAn2+NJBl/XfwuDsIfBzbqeatOPA2uTXAr8cFX9z1a//wzb/+/t/uzfYXBjr6sWsO/SWVty996RxhTgP1bVf/0HxcFvFJwYKp0ELmT07XdPZ+42/NvTeeGRvjTwJ8DPtd8lIMmqJFeeqnFV/R/gxSTXt9LWocUvMvjpSuk1x9CXgPZLSvcDjyZ5msG9588U3LcCu5I8yuDI/3ut/mUGH9wOf5ArvSZ4l01pTEkurqrvt+nbgZVV9bHz3C3ptDyvKI3vfUl2MPg7+ivgI+e3O9KZeaQvSR3xnL4kdcTQl6SOGPqS1BFDX5I6YuhLUkf+H9MxgnR//jzRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(sms[\"length\"], kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95239f75",
   "metadata": {},
   "source": [
    "# Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d915ff",
   "metadata": {},
   "source": [
    "Contraction Mapping/ Expanding Contraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9f8e9",
   "metadata": {},
   "source": [
    "It means it converts shortened form of word like I'll to I will. It helps in standardizing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c1ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "409a0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['no_contract'] = sms['msg'].apply(lambda x: [contractions.fix(word) for word in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edc6e952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \n",
       "0  Go until jurong point, crazy.. Available only ...  \n",
       "1                    Ok lar... Joking wif you oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...  \n",
       "3  YOU dun say so early hor... YOU c already then...  \n",
       "4  Nah I do not think he goes to usf, he lives ar...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[\"msg_str\"] = [' '.join(map(str, l)) for l in sms['no_contract']]\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094fdca8",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8749cb",
   "metadata": {},
   "source": [
    "Tokenization is breaking text into smaller parts, that smaller parts are called tokens, tokens can be words or sentences. It helps in understanding meaning of the text by analyizing tokens.\n",
    "https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f5000c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VIRAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "sms['tokenized'] = sms['msg_str'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92d0ae72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "      <td>[YOU, dun, say, so, early, hor, ..., YOU, c, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                    Ok lar... Joking wif you oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  YOU dun say so early hor... YOU c already then...   \n",
       "4  Nah I do not think he goes to usf, he lives ar...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...  \n",
       "1         [Ok, lar, ..., Joking, wif, you, oni, ...]  \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
       "3  [YOU, dun, say, so, early, hor, ..., YOU, c, a...  \n",
       "4  [Nah, I, do, not, think, he, goes, to, usf, ,,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2884cad",
   "metadata": {},
   "source": [
    "Noise Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeb7a40",
   "metadata": {},
   "source": [
    "It removes special character and punctuations which does not have any significant meaning are considered as noise and can be removed. It depends on use case to use case what should be considered as noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e2fc354",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['lower'] = sms['tokenized'].apply(lambda x: [word.lower() for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e22aec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy, .., avail...</td>\n",
       "      <td>[go, until, jurong, point, crazy, .., availabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "      <td>[YOU, dun, say, so, early, hor, ..., YOU, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                    Ok lar... Joking wif you oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  YOU dun say so early hor... YOU c already then...   \n",
       "4  Nah I do not think he goes to usf, he lives ar...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1         [Ok, lar, ..., Joking, wif, you, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor, ..., YOU, c, a...   \n",
       "4  [Nah, I, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [go, until, jurong, point, ,, crazy, .., avail...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                             no_punc  \n",
       "0  [go, until, jurong, point, crazy, .., availabl...  \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...  \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, he...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punc = string.punctuation\n",
    "sms['no_punc'] = sms['lower'].apply(lambda x: [word for word in x if word not in punc])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75442762",
   "metadata": {},
   "source": [
    "Spell Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8625f6d",
   "metadata": {},
   "source": [
    "Usually data can have many spelling mistakes. Its better to fix all the spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea04ffc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happening\n",
      "{'penning', 'henning', 'happening'}\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e1688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3cf44a",
   "metadata": {},
   "source": [
    "Removing Stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca6c51d",
   "metadata": {},
   "source": [
    "Stopwords are commonly occuring words which doesnot have much significance, therefore removing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39454a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\VIRAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>stopwords_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy, .., avail...</td>\n",
       "      <td>[go, until, jurong, point, crazy, .., availabl...</td>\n",
       "      <td>[go, jurong, point, crazy, .., available, bugi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "      <td>[YOU, dun, say, so, early, hor, ..., YOU, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[dun, say, early, hor, ..., c, already, say, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
       "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                    Ok lar... Joking wif you oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  YOU dun say so early hor... YOU c already then...   \n",
       "4  Nah I do not think he goes to usf, he lives ar...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1         [Ok, lar, ..., Joking, wif, you, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor, ..., YOU, c, a...   \n",
       "4  [Nah, I, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [go, until, jurong, point, ,, crazy, .., avail...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  [go, until, jurong, point, crazy, .., availabl...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, he...   \n",
       "\n",
       "                                   stopwords_removed  \n",
       "0  [go, jurong, point, crazy, .., available, bugi...  \n",
       "1              [ok, lar, ..., joking, wif, oni, ...]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "3  [dun, say, early, hor, ..., c, already, say, ...]  \n",
       "4     [nah, think, goes, usf, lives, around, though]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sms['stopwords_removed'] = sms['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355379e6",
   "metadata": {},
   "source": [
    "POS tagging and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7534750c",
   "metadata": {},
   "source": [
    "POS Tagging is a processing of assigning part of speech to each word in a sentence. It helps in understanding grammatical structure and meaning of text. For doing lemmatization we need to do POS Tagging and POS tag should be in wordnet format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e34302",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are methods to reduce words to their base form. In stemming the resuling base form word we get may not be lexicographically correct, while in lemmatization the words will be lexicographically correct. There stemming is faster and lemmtization is more accurate, it depends on use case what you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c06d881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\VIRAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#POS Tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "sms['pos_tags'] = sms['stopwords_removed'].apply(nltk.tag.pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96b5b4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\VIRAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy, .., avail...</td>\n",
       "      <td>[go, until, jurong, point, crazy, .., availabl...</td>\n",
       "      <td>[go, jurong, point, crazy, .., available, bugi...</td>\n",
       "      <td>[(go, VB), (jurong, JJ), (point, NN), (crazy, ...</td>\n",
       "      <td>[(go, v), (jurong, a), (point, n), (crazy, n),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, oni, ...]</td>\n",
       "      <td>[(ok, JJ), (lar, NN), (..., :), (joking, VBG),...</td>\n",
       "      <td>[(ok, a), (lar, n), (..., n), (joking, v), (wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[(free, JJ), (entry, NN), (2, CD), (wkly, JJ),...</td>\n",
       "      <td>[(free, a), (entry, n), (2, n), (wkly, a), (co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "      <td>[YOU, dun, say, so, early, hor, ..., YOU, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[dun, say, early, hor, ..., c, already, say, ...]</td>\n",
       "      <td>[(dun, NNS), (say, VBP), (early, JJ), (hor, NN...</td>\n",
       "      <td>[(dun, n), (say, v), (early, a), (hor, n), (.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
       "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[(nah, RB), (think, NN), (goes, VBZ), (usf, JJ...</td>\n",
       "      <td>[(nah, r), (think, n), (goes, v), (usf, a), (l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                    Ok lar... Joking wif you oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  YOU dun say so early hor... YOU c already then...   \n",
       "4  Nah I do not think he goes to usf, he lives ar...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1         [Ok, lar, ..., Joking, wif, you, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor, ..., YOU, c, a...   \n",
       "4  [Nah, I, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [go, until, jurong, point, ,, crazy, .., avail...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  [go, until, jurong, point, crazy, .., availabl...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, he...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [go, jurong, point, crazy, .., available, bugi...   \n",
       "1              [ok, lar, ..., joking, wif, oni, ...]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3  [dun, say, early, hor, ..., c, already, say, ...]   \n",
       "4     [nah, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(go, VB), (jurong, JJ), (point, NN), (crazy, ...   \n",
       "1  [(ok, JJ), (lar, NN), (..., :), (joking, VBG),...   \n",
       "2  [(free, JJ), (entry, NN), (2, CD), (wkly, JJ),...   \n",
       "3  [(dun, NNS), (say, VBP), (early, JJ), (hor, NN...   \n",
       "4  [(nah, RB), (think, NN), (goes, VBZ), (usf, JJ...   \n",
       "\n",
       "                                         wordnet_pos  \n",
       "0  [(go, v), (jurong, a), (point, n), (crazy, n),...  \n",
       "1  [(ok, a), (lar, n), (..., n), (joking, v), (wi...  \n",
       "2  [(free, a), (entry, n), (2, n), (wkly, a), (co...  \n",
       "3  [(dun, n), (say, v), (early, a), (hor, n), (.....  \n",
       "4  [(nah, r), (think, n), (goes, v), (usf, a), (l...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting in pos tag to wordnet format\n",
    "nltk.download('wordnet') \n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "sms['wordnet_pos'] = sms['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37b83cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>msg</th>\n",
       "      <th>length</th>\n",
       "      <th>no_contract</th>\n",
       "      <th>msg_str</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "      <th>no_punc</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>wordnet_pos</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>[Go, until, jurong, point,, crazy.., Available...</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[Go, until, jurong, point, ,, crazy, .., Avail...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy, .., avail...</td>\n",
       "      <td>[go, until, jurong, point, crazy, .., availabl...</td>\n",
       "      <td>[go, jurong, point, crazy, .., available, bugi...</td>\n",
       "      <td>[(go, VB), (jurong, JJ), (point, NN), (crazy, ...</td>\n",
       "      <td>[(go, v), (jurong, a), (point, n), (crazy, n),...</td>\n",
       "      <td>[go, jurong, point, crazy, .., available, bugi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>[Ok, lar..., Joking, wif, you, oni...]</td>\n",
       "      <td>Ok lar... Joking wif you oni...</td>\n",
       "      <td>[Ok, lar, ..., Joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, you, oni, ...]</td>\n",
       "      <td>[ok, lar, ..., joking, wif, oni, ...]</td>\n",
       "      <td>[(ok, JJ), (lar, NN), (..., :), (joking, VBG),...</td>\n",
       "      <td>[(ok, a), (lar, n), (..., n), (joking, v), (wi...</td>\n",
       "      <td>[ok, lar, ..., joke, wif, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[(free, JJ), (entry, NN), (2, CD), (wkly, JJ),...</td>\n",
       "      <td>[(free, a), (entry, n), (2, n), (wkly, a), (co...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>[YOU, dun, say, so, early, hor..., YOU, c, alr...</td>\n",
       "      <td>YOU dun say so early hor... YOU c already then...</td>\n",
       "      <td>[YOU, dun, say, so, early, hor, ..., YOU, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[you, dun, say, so, early, hor, ..., you, c, a...</td>\n",
       "      <td>[dun, say, early, hor, ..., c, already, say, ...]</td>\n",
       "      <td>[(dun, NNS), (say, VBP), (early, JJ), (hor, NN...</td>\n",
       "      <td>[(dun, n), (say, v), (early, a), (hor, n), (.....</td>\n",
       "      <td>[dun, say, early, hor, ..., c, already, say, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>[Nah, I, do not, think, he, goes, to, usf,, he...</td>\n",
       "      <td>Nah I do not think he goes to usf, he lives ar...</td>\n",
       "      <td>[Nah, I, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, ,,...</td>\n",
       "      <td>[nah, i, do, not, think, he, goes, to, usf, he...</td>\n",
       "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[(nah, RB), (think, NN), (goes, VBZ), (usf, JJ...</td>\n",
       "      <td>[(nah, r), (think, n), (goes, v), (usf, a), (l...</td>\n",
       "      <td>[nah, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                msg  length  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111   \n",
       "1   ham                      Ok lar... Joking wif u oni...      29   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155   \n",
       "3   ham  U dun say so early hor... U c already then say...      49   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61   \n",
       "\n",
       "                                         no_contract  \\\n",
       "0  [Go, until, jurong, point,, crazy.., Available...   \n",
       "1             [Ok, lar..., Joking, wif, you, oni...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor..., YOU, c, alr...   \n",
       "4  [Nah, I, do not, think, he, goes, to, usf,, he...   \n",
       "\n",
       "                                             msg_str  \\\n",
       "0  Go until jurong point, crazy.. Available only ...   \n",
       "1                    Ok lar... Joking wif you oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3  YOU dun say so early hor... YOU c already then...   \n",
       "4  Nah I do not think he goes to usf, he lives ar...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
       "1         [Ok, lar, ..., Joking, wif, you, oni, ...]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [YOU, dun, say, so, early, hor, ..., YOU, c, a...   \n",
       "4  [Nah, I, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                               lower  \\\n",
       "0  [go, until, jurong, point, ,, crazy, .., avail...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, ,,...   \n",
       "\n",
       "                                             no_punc  \\\n",
       "0  [go, until, jurong, point, crazy, .., availabl...   \n",
       "1         [ok, lar, ..., joking, wif, you, oni, ...]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [you, dun, say, so, early, hor, ..., you, c, a...   \n",
       "4  [nah, i, do, not, think, he, goes, to, usf, he...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [go, jurong, point, crazy, .., available, bugi...   \n",
       "1              [ok, lar, ..., joking, wif, oni, ...]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3  [dun, say, early, hor, ..., c, already, say, ...]   \n",
       "4     [nah, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                            pos_tags  \\\n",
       "0  [(go, VB), (jurong, JJ), (point, NN), (crazy, ...   \n",
       "1  [(ok, JJ), (lar, NN), (..., :), (joking, VBG),...   \n",
       "2  [(free, JJ), (entry, NN), (2, CD), (wkly, JJ),...   \n",
       "3  [(dun, NNS), (say, VBP), (early, JJ), (hor, NN...   \n",
       "4  [(nah, RB), (think, NN), (goes, VBZ), (usf, JJ...   \n",
       "\n",
       "                                         wordnet_pos  \\\n",
       "0  [(go, v), (jurong, a), (point, n), (crazy, n),...   \n",
       "1  [(ok, a), (lar, n), (..., n), (joking, v), (wi...   \n",
       "2  [(free, a), (entry, n), (2, n), (wkly, a), (co...   \n",
       "3  [(dun, n), (say, v), (early, a), (hor, n), (.....   \n",
       "4  [(nah, r), (think, n), (goes, v), (usf, a), (l...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [go, jurong, point, crazy, .., available, bugi...  \n",
       "1                [ok, lar, ..., joke, wif, oni, ...]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "3  [dun, say, early, hor, ..., c, already, say, ...]  \n",
       "4        [nah, think, go, usf, life, around, though]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "sms['lemmatized'] = sms['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f71bf4d",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b287338",
   "metadata": {},
   "source": [
    "Word Embedding are techniques which represents indibidual words as real-valued vector, where similar words will have similar words will have similar vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee67d19",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18decac3",
   "metadata": {},
   "source": [
    "Bag of Words is one of the popular word embedding technique. Each valued vector would represents count of words in a text. It does not contain information on the grammer or the order of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a2dcdfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['join_lemmatized']=sms['lemmatized'].map(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "68ae19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(sms['label'], sms['join_lemmatized'], test_size=0.3, stratify=sms['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd996f2b",
   "metadata": {},
   "source": [
    "since data is imbalanced, we are random oversampling the data, to balance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cc1f1f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "x,y = ros.fit_resample(x_train.array.reshape(-1, 1),y_train.array.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1a4cee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.flatten()\n",
    "y=y.flatten()\n",
    "d={'label':x,'join_lemmatized':y}\n",
    "df=pd.DataFrame(data=d,columns=['label','join_lemmatized'],index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e9c22a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>join_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spam</td>\n",
       "      <td>goal arsenal 4 henry 7 v liverpool 2 henry sco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>say go snow start around 8 9 pm tonite predict...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>aah bless arm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>oh k. come tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>happen gotten job begin registration permanent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84290</th>\n",
       "      <td>ham</td>\n",
       "      <td>  worry  finish march  ish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84291</th>\n",
       "      <td>ham</td>\n",
       "      <td>  worry  finish march  ish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84292</th>\n",
       "      <td>ham</td>\n",
       "      <td>  worry  finish march  ish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84293</th>\n",
       "      <td>ham</td>\n",
       "      <td>  worry  finish march  ish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84294</th>\n",
       "      <td>ham</td>\n",
       "      <td>  worry  finish march  ish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84295 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                    join_lemmatized\n",
       "0      spam  goal arsenal 4 henry 7 v liverpool 2 henry sco...\n",
       "1       ham  say go snow start around 8 9 pm tonite predict...\n",
       "2       ham                                      aah bless arm\n",
       "3       ham                                oh k. come tomorrow\n",
       "4       ham  happen gotten job begin registration permanent...\n",
       "...     ...                                                ...\n",
       "84290   ham                       worry  finish march  ish\n",
       "84291   ham                       worry  finish march  ish\n",
       "84292   ham                       worry  finish march  ish\n",
       "84293   ham                       worry  finish march  ish\n",
       "84294   ham                       worry  finish march  ish\n",
       "\n",
       "[84295 rows x 2 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4151b1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160     spam\n",
       "1690     ham\n",
       "4647    spam\n",
       "5090     ham\n",
       "2286     ham\n",
       "        ... \n",
       "2569     ham\n",
       "3971     ham\n",
       "1741    spam\n",
       "3411     ham\n",
       "1430    spam\n",
       "Name: label, Length: 1672, dtype: object"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7233e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_transformer = CountVectorizer().fit(df['join_lemmatized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "86d79021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<84295x6256 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 692093 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = bow_transformer.transform(df['join_lemmatized'])\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3b9ee",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a56962",
   "metadata": {},
   "source": [
    "Term Frequency-Inverse Document Frequency, it is a numerical statistic which shows how important a word is. </br>\n",
    "Term Frequency-Stores the information on how frequent a word is in a particular document. </br>\n",
    "Inverse Document Frequency-Store the information on how rare a word is in a document.</br>\n",
    "IDF is given more importance than TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bb51333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer().fit(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e1a0471c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<84295x6256 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 692093 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = tfidf_transformer.transform(bow)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ff289",
   "metadata": {},
   "source": [
    "Training using Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1b84bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(tfidf, df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a3d77c",
   "metadata": {},
   "source": [
    "Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cacc0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bow = bow_transformer.transform(y_test)\n",
    "test_tfidf = tfidf_transformer.transform(test_bow)\n",
    "prediction = spam_detect_model.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "24bd0e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1441    7]\n",
      " [  22  202]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(x_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a94c766b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1448\n",
      "        spam       0.97      0.90      0.93       224\n",
      "\n",
      "    accuracy                           0.98      1672\n",
      "   macro avg       0.98      0.95      0.96      1672\n",
      "weighted avg       0.98      0.98      0.98      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(x_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112fdbd",
   "metadata": {},
   "source": [
    "References:</br>\n",
    "1. U. (n.d.). spam_ham_detection/spam_ham_classification.ipynb at master  ujjwalgupta07/spam_ham_detection. GitHub. https://github.com/ujjwalgupta07/spam_ham_detection </br>\n",
    "2. Team, T. A. (2020, November 4). Natural Language Processing: Concepts and Workflow. Towards AI. https://towardsai.net/p/nlp/natural-language-processing-concepts-and-workflow-48083d2e3ce7 </br>\n",
    "3. pos tagging nlp. (n.d.). Shiksha. https://www.google.com/amp/s/www.shiksha.com/online-courses/articles/pos-tagging-in-nlp/amp </br>\n",
    "4. what are word embeddings. (n.d.). Machine Learning Mastery. https://machinelearningmastery.com/what-are-word-embeddings/ </br>\n",
    "5. Word embeddings in NLP: A Complete Guide. (n.d.). Word Embeddings in NLP: A Complete Guide. https://www.turing.com/kb/guide-on-word-embeddings-in-nlp</br>\n",
    "6. Chakravarthy, S. (2020, July 10). Tokenization for Natural Language Processing. Medium. https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4 </br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
